<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"allen108108.github.io","root":"/blog/","images":"/blog/images","scheme":"Gemini","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"width":280},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/blog/js/config.js"></script>
<meta name="description" content="摘要 Abstract Facebook AI 提出了一個新的物件偵測方法，這個方法簡化了整個偵測流程，且整個流程內不需要人工設置的部分，例如 : 非極大抑制演算法 (NMS, Non Maximun Suppression) 與 Anchor Box，這些必須事前人工設置的部分明確的將我們對於整個任務的先驗知識進行編碼。">
<meta property="og:type" content="article">
<meta property="og:title" content="[論文] End-to-End Object Detection with Transformers">
<meta property="og:url" content="https://allen108108.github.io/blog/2020/07/27/[%E8%AB%96%E6%96%87]%20End-to-End%20Object%20Detection%20with%20Transformers/index.html">
<meta property="og:site_name" content="Math.py">
<meta property="og:description" content="摘要 Abstract Facebook AI 提出了一個新的物件偵測方法，這個方法簡化了整個偵測流程，且整個流程內不需要人工設置的部分，例如 : 非極大抑制演算法 (NMS, Non Maximun Suppression) 與 Anchor Box，這些必須事前人工設置的部分明確的將我們對於整個任務的先驗知識進行編碼。">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://i.imgur.com/GWsrKFT.png">
<meta property="og:image" content="https://i.imgur.com/XnrEjMs.png">
<meta property="og:image" content="https://i.imgur.com/UxGHmsS.png">
<meta property="og:image" content="https://i.imgur.com/KhXBlBM.png">
<meta property="og:image" content="https://i.imgur.com/mzZD5Tu.png">
<meta property="og:image" content="https://i.imgur.com/ArBuCr7.png">
<meta property="og:image" content="https://i.imgur.com/cgqjhIy.jpg">
<meta property="og:image" content="https://i.imgur.com/rw4YvQN.png">
<meta property="og:image" content="https://i.imgur.com/8SdHhKa.png">
<meta property="og:image" content="https://i.imgur.com/0t55UFq.png">
<meta property="article:published_time" content="2020-07-27T09:08:24.000Z">
<meta property="article:modified_time" content="2020-07-28T00:54:09.466Z">
<meta property="article:author" content="Allen Tzeng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/GWsrKFT.png">


<link rel="canonical" href="https://allen108108.github.io/blog/2020/07/27/[%E8%AB%96%E6%96%87]%20End-to-End%20Object%20Detection%20with%20Transformers/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://allen108108.github.io/blog/2020/07/27/[%E8%AB%96%E6%96%87]%20End-to-End%20Object%20Detection%20with%20Transformers/","path":"2020/07/27/[論文] End-to-End Object Detection with Transformers/","title":"[論文] End-to-End Object Detection with Transformers"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[論文] End-to-End Object Detection with Transformers | Math.py</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149442581-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-149442581-1","only_pageview":false}</script>
  <script src="/blog/js/third-party/analytics/google-analytics.js"></script>




  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Math.py</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Wir müssen wissen , wir werden wissen</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li>
        <li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a></li>
        <li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81-abstract"><span class="nav-number">1.</span> <span class="nav-text">摘要 Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B0%A1%E4%BB%8B-introduce"><span class="nav-number">2.</span> <span class="nav-text">簡介 Introduce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E9%97%9C%E5%B7%A5%E4%BD%9C-related-work"><span class="nav-number">3.</span> <span class="nav-text">相關工作 Related work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E5%90%88%E9%A0%90%E6%B8%AC-set-prediction"><span class="nav-number">3.1.</span> <span class="nav-text">集合預測 Set Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer-%E8%88%87%E5%B9%B3%E8%A1%8C%E8%A7%A3%E7%A2%BC-transformers-and-parallel-decoding"><span class="nav-number">3.2.</span> <span class="nav-text">Transformer 與平行解碼 Transformers and Parallel Decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-object-detection"><span class="nav-number">3.3.</span> <span class="nav-text">物件偵測 Object detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%96%BC%E9%9B%86%E5%90%88%E7%9A%84%E6%90%8D%E5%A4%B1-set-based-loss"><span class="nav-number">3.3.1.</span> <span class="nav-text">基於集合的損失 Set-based loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%9E%E8%BF%B4%E5%81%B5%E6%B8%AC%E5%99%A8-recurrent-detectors"><span class="nav-number">3.3.2.</span> <span class="nav-text">遞迴偵測器 Recurrent detectors</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#detr-%E6%A8%A1%E5%9E%8B-the-detr-model"><span class="nav-number">4.</span> <span class="nav-text">DETR 模型 The DETR Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC%E9%9B%86%E5%90%88%E6%90%8D%E5%A4%B1-object-detection-prediction-loss"><span class="nav-number">4.1.</span> <span class="nav-text">物件偵測集合損失 Object detection prediction loss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%82%8A%E7%95%8C%E6%A1%86%E6%90%8D%E5%A4%B1-bounding-box-loss"><span class="nav-number">4.1.1.</span> <span class="nav-text">邊界框損失 Bounding box loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#detr-%E6%9E%B6%E6%A7%8B-detr-architecture"><span class="nav-number">4.2.</span> <span class="nav-text">DETR 架構 DETR architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AA%A8%E5%B9%B9-backbone"><span class="nav-number">4.2.1.</span> <span class="nav-text">骨幹 Backbone</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transformer-%E7%B7%A8%E7%A2%BC%E5%99%A8-transformer-encoder"><span class="nav-number">4.2.2.</span> <span class="nav-text">Transformer 編碼器 Transformer encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transformer-%E8%A7%A3%E7%A2%BC%E5%99%A8-transformer-decoder"><span class="nav-number">4.2.3.</span> <span class="nav-text">Transformer 解碼器 Transformer decoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A0%90%E6%B8%AC%E5%89%8D%E9%A5%8B%E7%B6%B2%E8%B7%AF-prediction-feed-forward-networks-ffns"><span class="nav-number">4.2.4.</span> <span class="nav-text">預測前饋網路 Prediction feed-forward networks (FFNs)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BC%94%E5%8A%A9%E8%A7%A3%E7%A2%BC%E6%90%8D%E5%A4%B1-auxiliary-decoding-loss"><span class="nav-number">4.2.5.</span> <span class="nav-text">輔助解碼損失 Auxiliary decoding loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%A6%E9%A9%97-experiments"><span class="nav-number">5.</span> <span class="nav-text">實驗 Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%88%87-faster-r-cnn-%E6%AF%94%E8%BC%83-comparison-with-faster-r-cnn"><span class="nav-number">5.1.</span> <span class="nav-text">與 Faster R-CNN 比較 Comparison with Faster R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E8%9E%8D%E5%AF%A6%E9%A9%97-ablations"><span class="nav-number">5.2.</span> <span class="nav-text">消融實驗 Ablations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B7%A8%E7%A2%BC%E5%B1%A4%E6%95%B8%E9%87%8F"><span class="nav-number">5.2.1.</span> <span class="nav-text">編碼層數量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E7%A2%BC%E5%B1%A4%E6%95%B8%E9%87%8F"><span class="nav-number">5.2.2.</span> <span class="nav-text">解碼層數量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ffn-%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">5.2.3.</span> <span class="nav-text">FFN 的重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%B7%A8%E7%A2%BC%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">5.2.4.</span> <span class="nav-text">位置編碼的重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%90%8D%E5%A4%B1%E6%B6%88%E8%9E%8D"><span class="nav-number">5.2.5.</span> <span class="nav-text">損失消融</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">5.3.</span> <span class="nav-text">分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#detr-%E4%BD%BF%E7%94%A8%E5%9C%A8%E5%85%A8%E6%99%AF%E5%88%86%E5%89%B2"><span class="nav-number">5.4.</span> <span class="nav-text">DETR 使用在全景分割</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B5%90%E8%AB%96-conclusion"><span class="nav-number">6.</span> <span class="nav-text">結論 Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A8%BB%E9%87%8B"><span class="nav-number">7.</span> <span class="nav-text">註釋</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Allen Tzeng"
      src="/blog/images/allen.jpg">
  <p class="site-author-name" itemprop="name">Allen Tzeng</p>
  <div class="site-description" itemprop="description">Study about Mathematics , Programming and Data Science</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blog/archives/">
          <span class="site-state-item-count">119</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blog/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://allen108108.github.io/" title="Github page → https:&#x2F;&#x2F;allen108108.github.io&#x2F;"><i class="github-alt fa-fw"></i>Github page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/allen108108" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;allen108108" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:allen108108@hotmail.com" title="E-Mail → mailto:allen108108@hotmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://allen108108.github.io/blog/2020/07/27/[%E8%AB%96%E6%96%87]%20End-to-End%20Object%20Detection%20with%20Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/allen.jpg">
      <meta itemprop="name" content="Allen Tzeng">
      <meta itemprop="description" content="Study about Mathematics , Programming and Data Science">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Math.py">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [論文] End-to-End Object Detection with Transformers
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2020-07-27 17:08:24" itemprop="dateCreated datePublished" datetime="2020-07-27T17:08:24+08:00">2020-07-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2020-07-28 08:54:09" itemprop="dateModified" datetime="2020-07-28T08:54:09+08:00">2020-07-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E8%AB%96%E6%96%87-Paper/" itemprop="url" rel="index"><span itemprop="name">論文 Paper</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E8%AB%96%E6%96%87-Paper/%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-Object-Detection/" itemprop="url" rel="index"><span itemprop="name">物件偵測 Object Detection</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="閱讀次數" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">閱讀次數：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/blog/2020/07/27/%5B%E8%AB%96%E6%96%87%5D%20End-to-End%20Object%20Detection%20with%20Transformers/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/07/27/[論文] End-to-End Object Detection with Transformers/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="摘要-abstract">摘要 Abstract</h2>
<p>Facebook AI 提出了一個新的物件偵測方法，這個方法簡化了整個偵測流程，且整個流程內不需要人工設置的部分，例如 : 非極大抑制演算法 (NMS, Non Maximun Suppression) 與 Anchor Box，這些必須事前人工設置的部分明確的將我們對於整個任務的先驗知識進行編碼。</p>
<span id="more"></span>
<p>本篇論文提出的物件偵測新框架的最主要架構稱之為 DEtection TRANSformer / DETR，是一種<strong>基於集合概念的全域損失</strong>，藉由<strong>二分匹配 ( Bipartite Matching )</strong> 與 <strong>Transformer 的 Encoder-Decoder 架構</strong>來強制進行唯一預測。給定一組固定的學習物件集合 Queries，DETR 會考慮到物件以及全域圖像上下文之間的關係，直接平行輸出最終預測集。</p>
<p>這個新型態的模型會跟現在 SOTA 的物件偵測器不同的地方是概念上來說較為簡單，不需要額外特別的 Library。DETR 與發展成熟且高度最佳化的 Faster R-CNN 相比，在準確度與整個效能來說都有很好的表現。而且，DETR 可以用一個統一的方式更泛化的進行全景分割。此論文指出此架構的表現高過現今具競爭力的基準。</p>
<p>訓練程式碼及預訓練模型均可於下列網址中取得 : https://github.com/facebookresearch/detr</p>
<h2 id="簡介-introduce">簡介 Introduce</h2>
<p>基本上，物件偵測就是對每一個我們感興趣的物件預測一組邊界框 (Bounding Box) 以及類別標註。現代的主流物件偵測，利用大量的候選框、Anchor Box 或是邊界框中心來定義替代回歸和分類問題，以此間接地解決這樣的任務。然而，這些偵測方式的效能、表現會因為 -- 後製 / 後處理來整合幾近重複的預測、Anchor Box 的設計以及目標邊界框分配給 Anchor Box 的方式而受到顯著的影響。</p>
<p>為了簡化這些流程，作者們提出了一個直接的集合預測方法來繞過替代任務。這種 End2End 的架構在複雜結構化預測任務中取得重大的進展，但在物件偵測領域上尚未取得任何進展 : 過去的偵測架構中，要不是用其他的型態添加了先驗知識，不然就是沒有被證明在具有挑戰性的強基準下具有足夠的競爭力。而本篇論文旨在彌補這一個差距。</p>
<p>作者將整個物件偵測視為是一個直接的集合預測問題，進而簡化了訓練流程。作者採用了基於 Transformer 的 Encoder-Decoder 架構，一種用於序列預測的流行架構。Transformer 中的自我注意力機制可以明確地將序列中個個元素之間的交互作用建模，使得這樣的結構特別適合用於具有特殊條件的集合預測，例如刪除重複的預測。</p>
<p><img src="https://i.imgur.com/GWsrKFT.png" /></p>
<p>DETR (上圖) 可以一次性的預測所有物件，並且透過特別的損失函數來進行 End2End 訓練，這種損失函數會進行預測對象與真實對象之間的二分匹配 (Bipartite Matching)。DETR 也刪除了多個人工設計的部分 (非極大抑制與 Anchor Box) 來簡化偵測流程，這些部分會對先驗知識進行編碼。與現有的物件偵測系統不同，DETR 不需要任何自定義層，因此可以在任何包含標準 CNN 及 Transformer class 的框架中輕易被重製。</p>
<p>與過去的直接集合預測研究比較，DETR 的主要特色是具有二分匹配損失以及帶有 (非自迴歸) 平行解碼的 Transformer 結構。相反的，先前的研究都專注在 RNN 自迴歸解碼。DETR 的匹配損失函數會分配給真實物件一個唯一的預測，並且不會改變預測物件的排列，這樣表示我們可以平行的進行預測。</p>
<p>這篇論文在最受歡迎的物件偵測資料集 COCO 與最具競爭力的物件偵測器 Faster R-CNN 進行比較藉此評估 DETR。從最初的發表開始，Faster R-CNN 經過了一次又一次的設計更動，其性能也越來越好。從本論文的實驗中顯示，DETR 具有能與之競爭的性能。準確地說，DETR 在大型的物件上顯著的優於 Faster R-CNN。這個實驗結果可能是由於 Transformer 的非局部計算所造成。然而，DETR 在小型物件上的檢測性能較低，作者認為未來的工作將會利用 Faster R-CNN 中的 FPN (Feature Pyramid Network) 來改善這個問題。</p>
<p>DETR 的訓練設定與現今標準物件偵測系統有許多不同之處。這個新的模型架構需要超長的訓練時間，並且得利於 Transformer 中的輔助解碼損失 (auxiliary decoding losses)。接下來，作者們將會探索哪些部份對於模型的性能至關重要。</p>
<p>DETR 模型的設計精神使其更容易應用到更多複雜的任務中。在論文的實驗中，左者們會展示使用預訓練的 DETR 在一個簡單的分割 head 部位進行訓練，得到的效能勝過全景分割上的基準。</p>
<h2 id="相關工作-related-work">相關工作 Related work</h2>
<p>這篇論文建立在許多領域先前的努力之上，其中包含 : 集合預測的二元匹配、Transformer 的 Encoder-Decoder 結構、平行解碼以及物件偵測。</p>
<h3 id="集合預測-set-prediction">集合預測 Set Prediction</h3>
<p>沒有任何的典型深度學習模型 (Canonical Deep Learning Model)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> 可以用來直接預測集合。最基本的集合預測任務就是多標籤分類 (Multilabel Classification)，這種任務的基準方法，one-vs-rest，也不適合用於偵測某個結構在許多元素之間的存在位置這類型的問題 ( 例如，幾乎相同的邊界框 )。</p>
<p>這類型的任務最困難的部分就是要避免重複的問題。現今大多數的物件偵測器都是使用非極大抑制法來後處理解決這樣的問題，但是如果要直接做集合預測就無法進行後處理。最好就是有一個全域推論框架，模型可以對所有待預測物件進行交互作用且避免冗餘，對於固定大小的集合預測，密集連接層固然可以做到，但是計算成本太高，一種比較通用的方式就是使用類似 RNN 這樣的自回歸模型。</p>
<p>不管在什麼狀況下，損失函數應該要對於預測物件的排列具不變性。一個通用的解決方法是使用基於匈牙利演算法<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>的損失函數，在預測與真實標註上進行二分匹配。這會強制排列不變，而且可以保證每一個目標都進有唯一的匹配。作者們遵循著二分匹配損失，但不使用自回歸模型，採用的是帶有平行解碼的 Transformer。</p>
<h3 id="transformer-與平行解碼-transformers-and-parallel-decoding">Transformer 與平行解碼 Transformers and Parallel Decoding</h3>
<p>Transformer 由 Vaswani 等人所提出，是一個新型基於注意力的機器翻譯模組。注意力機制是一個可以整合整個輸入序列資訊的神經網路結構。在 Transformer 中引入了類似非局部神經網路的自我注意力層，它可以掃過整個序列的每一個元素，並且藉由彙整後的資訊來進行更新。</p>
<p>這樣的方式其中一個優點在於基於注意力機制的模型會進行全域計算而且具有完善的記憶體儲存，與 RNN 相比，更適合用於長序列上。也因為如此，Transformer 在許多自然語言處理、語音處理或是電腦視覺問題上都取代了 RNN 的地位。</p>
<p>Transformer 一開始被用在自回歸模型中，遵循著早期的 Seq2Seq 模型，一個一個輸出生成 token。然而，過高的推論成本 ( 與輸出序列長度成正比，且難批次處理 ) 導致在音訊、機器翻譯、單字表徵學習 (Word Representation Learning) 與語音識別等領域中序列生成這件事情難以平行化處理。</p>
<p>作者們結合 Transformer 與平行解碼，以便在集合預測的計算成本與全域計算能力中進行權衡。</p>
<h3 id="物件偵測-object-detection">物件偵測 Object detection</h3>
<p>大多數現代化的物件偵測方法都會使用初始化的猜測來做預測。二階段式的偵測器，會針對候選框進行邊界框的預測，而一階段式的偵測器則是會針對 Anchor Box 或是可能存在物件中心的網格進行預測。在近期的研究都指出，這些偵測系統的性能有很大程度會取決於猜測初始化的方式。</p>
<p>在本篇論文提出的模型中，作者們針對輸入影像的絕對邊界框預測 (非 Anchor Box) 來直接進行集合預測，藉此來移除了這些人工設置的步驟且簡化整個偵測過程。</p>
<h4 id="基於集合的損失-set-based-loss">基於集合的損失 Set-based loss</h4>
<p>幾種物件偵測系統 (SSD / YOLO) 均使用了二分匹配損失。然而，這些早期的深度學習模型只使用了卷積層、全連接層來針對不同預測之間的關係進行建模，並且使用人工設置的 NMS 後處理方式來提高其性能。近期的偵測系統，配合著 NMS 針對真實標註與預測之間使用非唯一的分配規則。</p>
<p>可學習的 NMS 以及關係網路藉由注意力機制明確地將不同的預測之間的關係進行建模。使用直接的集合損失，不再需要任何後處理步驟。但是，這些方法必須使用額外人工設定的上下文特徵 (譬如，候選框座標)才能有效率地為偵測之間的關係建模，同時，作者們也在尋找減低模型中先驗知識編碼的解決方案。</p>
<h4 id="遞迴偵測器-recurrent-detectors">遞迴偵測器 Recurrent detectors</h4>
<p>最接近這篇論文所提出的方法的是用於物件偵測 End2End 的集合預測以及實例分割 (Instance Segmentation)，與作者們提出的方法類似，他們使用二分匹配損失配合基於 CNN 激化的 Encoder-Decoder 架構直接預測一組邊界框。但是這些方法目前僅有在小型資料集中進行評估，而沒有與現代基準進行比較。特別的地方是，他們是基於自回歸模型 ( 準確來說就是 RNN )，所以並沒有使用到近期平行解碼的 Transformer。</p>
<h2 id="detr-模型-the-detr-model">DETR 模型 The DETR Model</h2>
<p>要在偵測中直接做集合預測有兩個非常重要的關鍵 :</p>
<ol type="1">
<li>集合預測損失必須強制將預測框與真實框進行唯一匹配</li>
<li>整個架構必須要預測 (一次預測) 物件集合並且對其關係進行建模</li>
</ol>
<p>接下來便會針對整個架構進行細節上討論。</p>
<h3 id="物件偵測集合損失-object-detection-prediction-loss">物件偵測集合損失 Object detection prediction loss</h3>
<p>DETR 單一次通過 Decoder 後會進行固定尺寸的預測集，大小為 <span class="math inline">\(N\)</span> ，這個 <span class="math inline">\(N\)</span> 值會遠大於圖像中物件的數量。而訓練的困難點在於怎麼去針對預測的物件 ( 無論是物件分類、位置還是尺寸 ) 與真實物件給予評分。DETR 設計的損失會在預測物件與真實物件之間產生一個最佳的二分匹配，然後我們來藉此優化這個特定物件 (邊界框) 的損失。</p>
<p>假設 <span class="math inline">\(y\)</span> 為真實物件集合，而 <span class="math inline">\(\hat{y}=\{\hat{y}_i\}_{i=1}^N\)</span> 則是預測集合，其中 <span class="math inline">\(N\)</span> 值明顯大於圖像中的物件個數。我們將 <span class="math inline">\(y\)</span> 利用 padding <span class="math inline">\(\phi\)</span> (無物件數量) 擴增為尺寸為 <span class="math inline">\(N\)</span> 的集合。</p>
<p>為了找出這兩個集合 <span class="math inline">\(y, \hat{y}\)</span> 間的二分匹配，我們遍尋 <span class="math inline">\(N\)</span> 個元素排列 <span class="math inline">\(\sigma\in\mathfrak{S}_N\)</span> 中成本最低的，即 :</p>
<p><span class="math display">\[
\hat{\sigma}=\underset{\sigma\in\mathfrak{S}_N}{\arg\min}\sum_i^N\mathcal{L}_{\text{match}}\big(y_i,\hat{y}_{\sigma(i)}\big)
\]</span></p>
<p>其中 <span class="math inline">\(\mathcal{L}_{\text{match}}\big(y_i,\hat{y}_{\sigma(i)}\big)\)</span> 指的是真實 <span class="math inline">\(y_i\)</span> 與排列 <span class="math inline">\(\sigma\)</span> 中第 <span class="math inline">\(i\)</span> 個物件 <span class="math inline">\(\sigma(i)\)</span> 成對的匹配損失。上述的最佳化過程我們可以使用匈牙利演算法來有效率的求解。</p>
<p>匹配損失本身包含了類別預測以及真實邊界框與預測邊架框的相似度。每一個真實物件集合的元素 <span class="math inline">\(y_i\)</span> 都可以被視為 <span class="math inline">\((c_i, b_i)\)</span>，<span class="math inline">\(c_i\)</span> 是該物件的類別標籤 ( 也可能是 <span class="math inline">\(\phi\)</span> )，而 <span class="math inline">\(b_i\in[0,1]^4\)</span> 是一個四維向量，代表著邊界框中心點座標以及相對於圖像尺寸的長度與寬度。匹配損失在這篇論文中的定義如下 :</p>
<p><span class="math display">\[
\mathcal{L}_{\text{match}}\big(y_i,\hat{y}_{\sigma(i)}\big)=-\mathbb{I}_{\{c_i\neq\phi\}}\hat{p}_{\sigma(i)}(c_i)+\mathbb{I}_{\{c_i\neq\phi\}}\mathcal{L}_{\text{box}}(b_i,\hat{b}_{\sigma(i)})
\]</span></p>
<p>其中，<span class="math inline">\(\hat{p}_{\sigma(i)}(c_i)\)</span> 指的是在排列 <span class="math inline">\(\sigma\)</span> 中第 <span class="math inline">\(i\)</span> 個物件 <span class="math inline">\(\sigma(i)\)</span> 為 <span class="math inline">\(c_i\)</span> 的機率，而 <span class="math inline">\(\hat{b}_{\sigma(i)}\)</span> 則是指預測框。<strong>( 這個損失函數在第一眼見到的時候會覺得有些難懂，但事實上其實蠻簡單理解的，當這個 <span class="math inline">\(\sigma\)</span> 排列的第 <span class="math inline">\(i\)</span> 個類別為 <span class="math inline">\(c_i\)</span> 的機率越高時，損失會越低，而當邊界框的差異越大時，損失越高，將這兩者的損失值不賦予權重直接相加即為總損失 )</strong></p>
<p>這個尋找出最佳匹配的過程，其實就跟現代的物件偵測器中要去匹配候選框 (Proposals)、錨框 (Anchors) 到真實物件的角色一樣。最主要的區別在於 DETR 中的匹配過程必須一對一以直接進行沒有重複的集合預測。</p>
<p>第二步驟，利用匈牙利演算法來計算前一個步驟中每一個配對的總損失，作者將損失函數定義的類似於一般的物件偵測器，亦即，類別機率的負對數似然值與邊界框損失的線性組合 :</p>
<p><span class="math display">\[
\mathcal{L}_{\text{Hungarian}}(y,\hat{y})=\sum_{i=1}^{N}\Big[-\log \hat{p}_{\hat{\sigma}(i)}(c_i)+\mathbb{I}_{\{c_i\neq\phi\}}\mathcal{L}_{\text{box}}(b_i,b_{\hat{\sigma}(i)})\Big]
\]</span></p>
<p><span class="math inline">\(\hat{\sigma}\)</span> 指的是第一步驟所找出來的最佳分配。實務上，當 <span class="math inline">\(c=\phi\)</span> 時，會將機率對數值的權重降低十倍，以解決分類上的不平衡。<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>這樣的方法類似於 Faster R-CNN 中利用次採樣 (Subsampling) 來平衡正負候選框 (Proposals)。</p>
<p>值得注意的地方是，物件與 <span class="math inline">\(\phi\)</span> 的匹配成本並不取決於預測，也就是說，在這種情況下匹配成本是一個常數。再這樣的匹配成本中，利用機率值 <span class="math inline">\(\hat{p}_{\hat{\sigma}(i)}(c_i)\)</span> 取代機率對數值。這樣的方式會使分類預測項可與 <span class="math inline">\(\mathcal{L}_{\text{box}}(\cdot,\cdot)\)</span> 相稱 (後面會進行解釋)，並且作者也觀察到這樣會有更好的實證表現。</p>
<h4 id="邊界框損失-bounding-box-loss">邊界框損失 Bounding box loss</h4>
<p>匹配損失與匈牙利損失中的第二項 <span class="math inline">\(\mathcal{L}_{\text{box}}(\cdot)\)</span> 指的就是邊界框的分數。與其他的偵測器將邊界框預測視為針對初始猜測的偏移不同，DETR 是直接進行邊界框的預測。儘管這樣的方法可以簡化整個預測，但卻同時會造成損失相對放大的問題。</p>
<p>最常見的 <span class="math inline">\(\ell_1\)</span> 損失在不同大小的邊界框中，即使相對誤差接近也會有比例上的差異。為了解決這樣的問題，作者利用 <span class="math inline">\(\ell_1\)</span> 損失與廣義的 <span class="math inline">\(IOU\)</span> 損失 <span class="math inline">\(\mathcal{L}_{\text{iou}}(\cdot,\cdot)\)</span> 的線性組合，這樣的設計不會因為尺度變化而有所改變 (Scale-invariant)。</p>
<p>總而言之，我們將邊界框損失 <span class="math inline">\(\mathcal{L}_{\text{box}}(b_i,\hat{b}_{\sigma(i)})\)</span> 定義為 :</p>
<p><span class="math display">\[
\mathcal{L}_{\text{box}}(b_i,\hat{b}_{\sigma(i)})=\lambda_{\text{iou}}\mathcal{L}_{\text{iou}}(b_i,\hat{b}_{\sigma(i)})+\lambda_{\text{L}_1}\|b_i-\hat{b}_{\sigma(i)}\|_1
\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{\text{iou}},\lambda_{\text{L}_1}\in\mathbb{R}\)</span> 為超參數 (hyperparameters)，而這兩個損失則是利用每個批量中的物件數量進行標準化 (Normalized)。</p>
<h3 id="detr-架構-detr-architecture">DETR 架構 DETR architecture</h3>
<p>整個 DETR 架構十分簡單，如下圖 (Fig. 2) 所示。它包含了三個部分 : 以 CNN 為主體的骨幹、Transformer 以及一個簡單的前饋網路 (FFN, Feed forward Network)，最後作出偵測結果。</p>
<p><img src="https://i.imgur.com/XnrEjMs.png" /></p>
<p>與現今許多物件偵測器不同，DETR 可以在任何提供通用 CNN 與數百行程式碼的 Transformer 架構的深度學習框架中實現，甚至可以在少於 50 行程式碼的 Pytorch 實現推論。作者希望這個方法可以吸引到新的研究人員進入到物件偵測的領域。</p>
<p><img src="https://i.imgur.com/UxGHmsS.png" /></p>
<h4 id="骨幹-backbone">骨幹 Backbone</h4>
<p>從初始圖像 <span class="math inline">\(X_{\text{img}}\in \mathbb{R}^{3\times H_0\times W_0}\)</span> 開始，一個卷積 CNN 骨幹會生成一個低解析度的 activation map <span class="math inline">\(f\in \mathbb{R}^{C\times H\times W}\)</span>。論文內的設置為 <span class="math inline">\(C=2048, H=\dfrac{H_0}{32}, W=\dfrac{W_0}{32}\)</span>。</p>
<h4 id="transformer-編碼器-transformer-encoder">Transformer 編碼器 Transformer encoder</h4>
<p>首先，利用 <span class="math inline">\(1\times 1\)</span> 卷積來降低 channel 維度，由 <span class="math inline">\(C\)</span> 降至 <span class="math inline">\(d\)</span>，生成一個新的特徵圖 <span class="math inline">\(z_0\in \mathbb{R}^{d\times H\times W}\)</span>。這個編碼器需要用一個序列作為輸入，因此必須將 <span class="math inline">\(z_0\)</span> 的維度變成一個維度，得到一個 <span class="math inline">\(d\times HW\)</span> 特徵圖。每一個編碼層都有一個標準架構且包含一個多頭自注意力機制與一個前饋網路 ( FFN, Feed Forward Network )。</p>
<p>由於 Transformer 是一個無關序列元素位置的機制 ( Permutation-invariant, 具有置換不變性 )，因此在每一個注意力層的輸入中加入一組固定的位置編碼。此篇論文也將整個注意力機制的詳細定義補充於論文末，作為補充材料。</p>
<h4 id="transformer-解碼器-transformer-decoder">Transformer 解碼器 Transformer decoder</h4>
<p>解碼層部分遵循著 Transformer 的標準架構，使用多頭自注意力機制轉換成 大小為 <span class="math inline">\(d\)</span> 的 <span class="math inline">\(N\)</span> 個嵌入 ( Embeddings )。與原始的 Transformer 不同的地方在於，DETR 會在解碼層中平行解碼出 <span class="math inline">\(N\)</span> 個物件，而原始的 Transformer 則是使用自迴歸模型一次僅預測出一個元素的輸出序列。同樣的，這個部分的詳細內容也記錄於論文末的補充材料中。</p>
<p>由於解碼層也具有置換不變性，因此在解碼層的 <span class="math inline">\(N\)</span> 個輸入嵌入 ( input embeddings ) 中也必須加入位置編碼的資訊，而這個位置編碼是經由學習而來的，稱之為 Object Query。如同編碼層般，Object Query 便會加入到每一個注意力層中。</p>
<p>這 <span class="math inline">\(N\)</span> 個 Object Queries 經由解碼層輸出成一組輸出嵌入 ( Output embedding )。這些輸出會獨立地經由前饋網路 ( FFN ) 來解碼成 <span class="math inline">\(N\)</span> 組邊界框座標及類別標籤預測。</p>
<p>使用自注意力與編-解碼注意力機制，這樣的模型可以對所有的物件進行全局推理，同時能使用整張圖像的脈絡。</p>
<h4 id="預測前饋網路-prediction-feed-forward-networks-ffns">預測前饋網路 Prediction feed-forward networks (FFNs)</h4>
<p>最終的預測是經由 3 層帶有 ReLU activation function 以及隱藏維度 <span class="math inline">\(d\)</span> 的 perceptron 加上一個線性層計算而得。這樣的 FFN 會預測出針對輸入圖像的邊界框中心座標及長寬，而線性層則是利用 softmax 來直接預測類別。</p>
<p>因為我們預測了一組固定尺寸為 <span class="math inline">\(N\)</span> 的邊界框，而這 <span class="math inline">\(N\)</span> 的數量通常比圖像中的真實物件數量大的多，因此 DETR 中使用額外的標籤 <span class="math inline">\(\phi\)</span> 來表示沒有物件被偵測到。這種類別標註就跟一般物件偵測器中的「背景」類別意義一樣。</p>
<h4 id="輔助解碼損失-auxiliary-decoding-loss">輔助解碼損失 Auxiliary decoding loss</h4>
<p>作者發現，訓練過程中在解碼層中使用輔助損失 ( Auxiliary loss ) 會非常有用，尤其是對於模型輸出每個物件的正確數量更有幫助。作者在每一個解碼層後加入了 FFNs 以及匈牙利損失。所有的 FFNs 均共享權重。作者也使用了額外的共享 Layer-Normalize 層來對於不同解碼層的 FFNs 輸入進行標準化。</p>
<h2 id="實驗-experiments">實驗 Experiments</h2>
<p>(略)</p>
<h3 id="與-faster-r-cnn-比較-comparison-with-faster-r-cnn">與 Faster R-CNN 比較 Comparison with Faster R-CNN</h3>
<p>(略)</p>
<h3 id="消融實驗-ablations">消融實驗 Ablations</h3>
<p>Transformer 解碼層中的注意力機制對於不同偵測內的特徵表徵之間的關係建模是非常關鍵的部分。在論文中的消融實驗分析中，作者探索模型架構中不同部分及損失是如何影響最終的模型表現。</p>
<p>為了進行消融實驗，作者利用基於 ResNet-50 的 DETR 模型，這個模型中具有 6 個編碼層、6 個解碼層且寬度為 256。這個模型具有 41.3 M 個參數，分別在短期、長期行程中分別達到 40.6 及 42.0 AP，並達到 28 FPS，接近具有相同骨幹的 Faster R-CNN-FPN。</p>
<h4 id="編碼層數量">編碼層數量</h4>
<p>這部分，作者藉由改變編碼層數量來評估全局圖像層級的自注意力機制的重要性。( 見 Table 2. )</p>
<p><img src="https://i.imgur.com/KhXBlBM.png" /></p>
<p>當完全沒有編碼層時，整體 AP 下降 3.9，在大物件偵測的 AP 上更是下降達6.0。在這樣的狀況下，作者假設，透過全局圖像的推理，編碼層對於分離物件具有很大的重要性。</p>
<p>在下圖 ( Fig. 3 ) 中，作者將預訓練模型中最後一層編碼層的注意力圖層視覺化，關注在圖像中幾個特定點上。我們可以發現，編碼層似乎已經分離了物件實體，而這樣的過程可能也簡化了解碼層中對物件的萃取及定位。</p>
<p><img src="https://i.imgur.com/mzZD5Tu.png" /></p>
<h4 id="解碼層數量">解碼層數量</h4>
<p>前面有提到，作者在每一個解碼層後面應用了輔助解碼損失 ( Auxiliary decoding loss ) ，因此，訓練預測用的 FFNs 被設計從每一個解碼層的輸出來預測物件。藉由評估解碼層中每一個階段會被預測的物件來分析每一個解碼層的重要性 ( 見下圖 Fig. 4 )。</p>
<p><img src="https://i.imgur.com/ArBuCr7.png" /></p>
<p><span class="math inline">\(AP\)</span> 與 <span class="math inline">\(AP_{50}\)</span> 每經過一層解碼器都會有所增進，第一層跟最後一層解碼層之間的 <span class="math inline">\(AP\)</span> 顯著增進了 <span class="math inline">\(8.2 / 9.5\)</span>。因為 DETR 具有基於集合的損失，因此不需使用設計好的 NMS。為了驗證這樣的觀點，作者在每個解碼層後使用預設參數的標準 NMS。NMS 在第一層解碼層中改善了預測性能。這項結果可以透過以下的事實來解釋 : 因為 Transformer 中的解碼層無法計算出輸出元素的交叉相關 ( Cross-Correlation )，因此傾向於對同一個物件進行多次預測，但在第二層及後續的解碼層中，經過 activation 的自注意力機制會使模型抑制重複的預測。從上圖 ( Fig. 4 ) 也可以觀察到， NMS 所帶來的性能增進會隨著解碼層數量而逐漸變少。在最後一層中，經過 NMS 後甚至會有少量的 <span class="math inline">\(AP\)</span> 下降情況，這是因為 NMS 反而錯誤的刪去了真實的陽性預測 ( Positive Predictions )。</p>
<p>類似於編碼層的注意力機制視覺化，作者將解碼層的注意力機制可視化 ( Fig. 6 ) ，並將每一個預測物件在注意力圖層上給予不同的顏色表示。作者發現到，解碼層的注意力機制非常的局部，意指它大多將注意力集中於物件的邊緣部分，例如頭或腳。作者對此假設，編碼層透過注意力機制將物件進行分離之後，解碼層則利用注意力機制再利用肢體、邊緣等來進行類別的確認及物件的邊界。</p>
<p><img src="https://i.imgur.com/cgqjhIy.jpg" /></p>
<h4 id="ffn-的重要性">FFN 的重要性</h4>
<p>在 Transformer 中的 FFN 可以視為是一個 <span class="math inline">\(1\times 1\)</span> 卷積層，讓整個編碼過程類似於一個帶有注意力增強的 CNN。作者嘗試將其刪除，只保留注意力層，參數量從 41.3 M 減少至 28.7 M ，但性能卻下降了 2.3 AP，從這裡可以知道， FFN 對於達到良好的結果具有其重要性。</p>
<h4 id="位置編碼的重要性">位置編碼的重要性</h4>
<p>DETR 中有兩種位置編碼 : 空間位置編碼以及輸出位置編碼 ( Object Queries )。作者在實驗中嘗試了多種固定的、學習而來的位置編碼組合，如下表 ( table 3 )。</p>
<p><img src="https://i.imgur.com/rw4YvQN.png" /></p>
<p>輸出位置編碼是必須的，無法刪除，因此作者實驗在解碼層輸入的部分或在每個解碼注意力層的 Query 中加入空間位置編碼。在第一個實驗中，完全刪除空間位置編碼，僅保留輸出位置編碼在輸入處。有趣的是，這個模型仍然達到了 32 AP，與基準相比下降了 7.8 AP。其次，與原始的 Transformer 相同，使用 sine 空間位置編碼一次傳遞到輸入端，可以發現，相較於基準將 sine 空間位置編碼一次傳遞到注意力層中，這樣會導致 1.4 AP 的下降。將學習到的空間位置編碼傳遞到注意力層會得到類似的結果。不過令人驚訝的是，當作者刪除編碼層的空間位置編碼，竟然只導致 1.3 AP 的下降。最後，傳遞空間位置編碼到注意力層，在所有層裡面共享，而輸出位置編碼 ( Object Queries ) 則是學習而來的。</p>
<p>上述的這些消融實驗，可以證明 Transformer 的各個部分 ( 編碼層中的全局自注意力機制、FFN、多編碼層以及位置編碼 ) 都對於物件偵測的性能有著重要的貢獻。</p>
<h4 id="損失消融">損失消融</h4>
<p>為了評估匹配損失及損失函數不同部分的重要性，作者訓練了多種模型，分別開啟或關閉這些部分來看看有什麼影響。</p>
<p>從損失函數來看，一共包含了三個部分 : 分類損失、<span class="math inline">\(\ell_1\)</span> 邊界框距離損失以及 <span class="math inline">\(GIoU\)</span> 損失。分類損失是必要的，無法刪除，因此作者拿掉邊界框距離損失訓練一個模型、拿掉 <span class="math inline">\(GIoU\)</span> 損失再訓練一個模型，並與基準保留三個部份訓練出一個模型進行比較，結果呈現在下表 ( tabel 4 )。</p>
<p><img src="https://i.imgur.com/8SdHhKa.png" /></p>
<p><span class="math inline">\(GIoU\)</span> 損失對於模型性能中是最主要的部份，如果結合分類損失，整個模型僅下降了 0.7 AP，但若使用邊界框損失而不使用 <span class="math inline">\(GIoU\)</span> 損失則會得到很差的結果。作者僅僅研究不同損失之間的消融狀況 ( 每次都使用相同的權重 )，但是如果將這些損失結合再一起可能會達到不同的結果。</p>
<h3 id="分析">分析</h3>
<p>(略)</p>
<h3 id="detr-使用在全景分割">DETR 使用在全景分割</h3>
<p>全景分割任務於近年來在電腦視覺領域中獲得廣泛的關注。類似於 Faster R-CNN 擴展成為 Mask R-CNN ，DETR 也可以於解碼層的輸出頂層加入遮罩層 ( Mask head ) 來擴展 DETR。在這一部分，作者展示了這樣的方式可以統一用來製造出全景分割。作者在 COCO 資料集的全景標註中進行實驗，該資料集中除了 80 種可數物件種類外還有 53 種不可數物件種類。</p>
<p>作者先使用 COCO 資料集中可數物件與不可數物件類別訓練原先的 DETR 模型來預測邊界框。預測邊界框這個步驟是必要的，因為匈牙利演算法要計算出邊界框的距離。除此之外，作者在解碼層中添加了一個遮罩層，針對每一個邊界框進行二元遮罩，如下圖 ( Fig. 8 )。</p>
<p><img src="https://i.imgur.com/0t55UFq.png" /></p>
<p>這個遮罩層以每一個解碼層的輸出作為輸入，並且根據編碼層的輸出嵌入所計算出來的多 (<span class="math inline">\(M\)</span>) 頭注意力分數來生成 <span class="math inline">\(M\)</span> 張低解析度的熱圖。為了產出最終結果並且提高解析度，作者還使用了類似 FPN (Feature Pyramid Network) 的結構。( 這在論文末的補充材料中會有更細節的介紹 ) 最終遮罩的解析度提高四倍，且每一個遮罩都使用 <span class="math inline">\(DICE/F-1\)</span> 損失函數進行監督式學習。</p>
<p>遮罩層的訓練可以一次完成，也可以分兩階段進行，先訓練原始的 DETR 模型預測邊界框，再使用這些權重來訓練遮罩層 25 epochs。實驗證實，這兩種方法得到的最終結果相差不遠。</p>
<p>(略)</p>
<h2 id="結論-conclusion">結論 Conclusion</h2>
<p>這篇論文中提出了 DETR，一種基於 Transformers 與二分匹配損失的全新物件偵測系統，可以直接進行及和預測。在 COCO 資料集上，DETR 可以獲得與最佳化過後的 Faster R-CNN 接近的結果。DETR 容易實現，而且具有彈性的架構可以輕易擴展到全景分割任務中，並且獲得不錯的結果。除此之外，DETR 在大型物件上的表現比 Faster R-CNN 更好，或許這可以歸功於自我注意力機制的全局訊息處理上。</p>
<p>但是這全新的物件偵測系統也迎來新的挑戰，尤其是在小物件上的訓練、最佳化及性能上。現今的偵測系統都需要幾年的時間改進來應付類似的問題，作者也期待在未來的努力上 DETR 可以成功解決這些問題。</p>
<h2 id="註釋">註釋</h2>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>典型模型 Canonical Model : 典型模型指的是一種更高層級的設計模式，可以藉由模組與不同模型之間進行資料交互，換個角度來說，我們也可以稱典型模型是一般模型的 superset。<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>匈牙利演算法 Hungarian Algorithm : 一種建立在匈牙利數學家 Dénes Kőnig和Jenő Egerváry 的研究之上的演算法，專門處理在多項式時間內求解任務分配問題的組合演算法。<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>細節可以參考 &quot;<a href="https://allen108108.github.io/blog/2019/11/24/%5B%E8%AB%96%E6%96%87%5D%20You%20Only%20Look%20Once%20_%20Unified,%20Real-Time%20Object%20Detection/">[論文] You Only Look Once : Unified, Real-Time Object Detection</a> &quot; 一文。<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</section>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>作者： </strong>Allen Tzeng
  </li>
  <li class="post-copyright-link">
      <strong>文章連結：</strong>
      <a href="https://allen108108.github.io/blog/2020/07/27/[%E8%AB%96%E6%96%87]%20End-to-End%20Object%20Detection%20with%20Transformers/" title="[論文] End-to-End Object Detection with Transformers">https://allen108108.github.io/blog/2020/07/27/[論文] End-to-End Object Detection with Transformers/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版權聲明： </strong>本網誌所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 許可協議。轉載請註明出處！
  </li>
</ul>
</div>


        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog/2020/06/26/%E8%8A%8B%E9%A6%99%E5%86%AC%E7%93%9C%E3%80%8C%E6%9F%A5%E3%80%8D%20_%20%E5%B0%88%E7%82%BA%E8%87%AA%E5%AE%B6%E9%9C%80%E6%B1%82%E7%9A%84%E6%A9%9F%E5%99%A8%E4%BA%BA%E6%9F%A5%E8%A9%A2%E7%B3%BB%E7%B5%B1%20App%20(%20MoBot%20ver.2%20)/" rel="prev" title="芋香冬瓜「查」 : 專為自家需求的機器人查詢系統 App ( MoBot ver.2 ) ">
                  <i class="fa fa-chevron-left"></i> 芋香冬瓜「查」 : 專為自家需求的機器人查詢系統 App ( MoBot ver.2 ) 
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/blog/2020/09/05/%E5%B0%87%E5%B7%B2%E6%9C%89%E7%9A%84%E5%B0%88%E6%A1%88%E5%8A%A0%E5%85%A5%20TotoiseSVN%20%E9%80%B2%E8%A1%8C%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/" rel="next" title="將已有的專案加入 TotoiseSVN 進行版本控制">
                  將已有的專案加入 TotoiseSVN 進行版本控制 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allen Tzeng</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="訪客總數">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="總瀏覽次數">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/next-boot.js"></script><script src="/blog/js/pjax.js"></script>

  
  <script src="https://embed.widgetpack.com/widget.js" async></script>
  <script class="next-config" data-name="rating" type="application/json">{"enable":true,"id":21351,"color":"#fc6423"}</script>
  <script src="/blog/js/third-party/rating.js"></script>




  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/blog/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"math-py","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/blog/js/third-party/comments/disqus.js"></script>

</body>
</html>
